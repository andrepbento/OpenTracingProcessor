{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience\n",
    "\n",
    "This experience shows an approach used to perform Time-Series data analysis involving Machine Learning.\n",
    "\n",
    "Considerations:\n",
    "- Multiple features to be analyzed;\n",
    "- Possible correlation between features;\n",
    "- Unlabeled data;\n",
    "- (Un)supervised learning;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code bellow was extracted from a module, opentsdb client, presented in Graphy.\n",
    "This function retrieves data from OpenTSDB using the REST API defined by the database developers. To retrieve data, the metric name, start timestamp and end timestamp must be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "opentsdb_address = 'http://127.0.0.1:4242/'\n",
    "api_query = 'api/query'\n",
    "\n",
    "def get_metrics(name: str, start_timestamp: int, end_timestamp: int) -> dict:\n",
    "    \"\"\"\n",
    "    Gets the metrics from OpenTSDB.\n",
    "\n",
    "    :param name: The name of the metrics.\n",
    "    :param start_timestamp: The start unix timestamp of the metric.\n",
    "    :param end_timestamp: The end unix timestamp of the metric.\n",
    "    :return: The metrics as a dictionary if success, None otherwise.\n",
    "    \"\"\"\n",
    "    json_body = {\n",
    "        \"start\": start_timestamp,\n",
    "        \"end\": end_timestamp,\n",
    "        \"queries\": [{\"aggregator\": \"sum\", \"metric\": name},\n",
    "                    {\"aggregator\": \"sum\", \"tsuids\": [\"000001000002000042\", \"000001000002000043\"]}]\n",
    "    }\n",
    "\n",
    "    data = None\n",
    "    try:\n",
    "        response = requests.post(opentsdb_address + api_query, data=json.dumps(json_body),\n",
    "                                 headers={'content-type': 'application/json'})\n",
    "        if response.status_code == 200:\n",
    "            response_text = json.loads(response.text)\n",
    "            if response_text:\n",
    "                data = response_text[0].get('dps', None)\n",
    "        return data\n",
    "    except ConnectionError as ex:\n",
    "        logger.error('{}: {}'.format(type(ex), ex))\n",
    "        sys.exit(status=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data to be analyzed\n",
    "\n",
    "The data is gathered from a Time-Series database (OpenTSDB). In this case we get some metrics from this kind of database to perform the analysis.\n",
    "\n",
    "We are using the OUTER JOIN method to merge data from multiple features. This merge method preserves the data points and fills the missing values with NaN (Missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1341 entries, 0 to 1340\n",
      "Data columns (total 5 columns):\n",
      "time               1341 non-null object\n",
      "call_count         1341 non-null int64\n",
      "status_code_2xx    1274 non-null float64\n",
      "status_code_4xx    1168 non-null float64\n",
      "status_code_5xx    8 non-null float64\n",
      "dtypes: float64(3), int64(1), object(1)\n",
      "memory usage: 62.9+ KB\n",
      "\n",
      "Data info:\n",
      "None\n",
      "\n",
      "Data:\n",
      "            time  call_count  status_code_2xx  status_code_4xx  \\\n",
      "0     1530137700         758         0.280543         0.719457   \n",
      "1     1530138135         271         0.388571         0.611429   \n",
      "2     1530138150         676         0.295000         0.705000   \n",
      "3     1530138165         405         0.222222         0.777778   \n",
      "4     1530138195          82         0.142857         0.857143   \n",
      "5     1530138210          82         0.142857         0.857143   \n",
      "6     1530138300         758         0.280543         0.719457   \n",
      "7     1530138600         758         0.280543         0.719457   \n",
      "8     1530140670         878         0.849558         0.150442   \n",
      "9     1530140685         878         0.849558         0.150442   \n",
      "10    1530140700        1366         0.712519         0.287481   \n",
      "11    1530140715         418         0.234043         0.765957   \n",
      "12    1530140730         488         0.209386         0.790614   \n",
      "13    1530140745          70         0.071429         0.928571   \n",
      "14    1530141300        1366         0.712519         0.287481   \n",
      "15    1530142200        2228         0.607492         0.392508   \n",
      "16    1530143100         862         0.359489         0.640511   \n",
      "17    1530143415          89         0.176471         0.823529   \n",
      "18    1530143430         502         0.430168         0.569832   \n",
      "19    1530143445         413         0.472313         0.527687   \n",
      "20    1530143475         360         0.226316         0.773684   \n",
      "21    1530143490         360         0.226316         0.773684   \n",
      "22    1530143700         862         0.359489         0.640511   \n",
      "23    1530145800         819         0.185430         0.814570   \n",
      "24    1530146100         819         0.185430         0.814570   \n",
      "25    1530146250         242         0.217391         0.782609   \n",
      "26    1530146265         242         0.217391         0.782609   \n",
      "27    1530146295         355         0.253968         0.746032   \n",
      "28    1530146310         577         0.171429         0.828571   \n",
      "29    1530146325         222         0.047619         0.952381   \n",
      "...          ...         ...              ...              ...   \n",
      "1311  1530269100         501         0.433628         0.566372   \n",
      "1312  1530269115         292         0.436464         0.563536   \n",
      "1313  1530269130         342         0.402844         0.597156   \n",
      "1314  1530269145          50         0.200000         0.800000   \n",
      "1315  1530269475          99         0.120690         0.879310   \n",
      "1316  1530269490         282         0.245509         0.754491   \n",
      "1317  1530269505         183         0.311927         0.688073   \n",
      "1318  1530269535         118         0.045455         0.954545   \n",
      "1319  1530269550         118         0.045455         0.954545   \n",
      "1320  1530269700         951         0.373913         0.626087   \n",
      "1321  1530269790         252         0.664260         0.335740   \n",
      "1322  1530269805         252         0.664260         0.335740   \n",
      "1323  1530269835         256         0.150943         0.849057   \n",
      "1324  1530269850         299         0.166667         0.833333   \n",
      "1325  1530269865          43         0.285714         0.714286   \n",
      "1326  1530270195          45         0.111111         0.888889   \n",
      "1327  1530270210         427         0.671772         0.328228   \n",
      "1328  1530270225         382         0.706977         0.293023   \n",
      "1329  1530270255        1115         0.930647         0.069353   \n",
      "1330  1530270270        1115         0.930647         0.069353   \n",
      "1331  1530270300        1542         0.870624         0.129376   \n",
      "1332  1530270630          41         0.307692         0.692308   \n",
      "1333  1530270645          41         0.307692         0.692308   \n",
      "1334  1530270675         201         0.080645         0.919355   \n",
      "1335  1530270690         361         0.273723         0.726277   \n",
      "1336  1530270705         160         0.433333         0.566667   \n",
      "1337  1530270900         511         0.257703         0.742297   \n",
      "1338  1530271155         109         0.157895         0.842105   \n",
      "1339  1530271170         109         0.157895         0.842105   \n",
      "1340  1530271800        2053         0.776632         0.223368   \n",
      "\n",
      "      status_code_5xx  \n",
      "0                 NaN  \n",
      "1                 NaN  \n",
      "2                 NaN  \n",
      "3                 NaN  \n",
      "4                 NaN  \n",
      "5                 NaN  \n",
      "6                 NaN  \n",
      "7                 NaN  \n",
      "8                 NaN  \n",
      "9                 NaN  \n",
      "10                NaN  \n",
      "11                NaN  \n",
      "12                NaN  \n",
      "13                NaN  \n",
      "14                NaN  \n",
      "15                NaN  \n",
      "16                NaN  \n",
      "17                NaN  \n",
      "18                NaN  \n",
      "19                NaN  \n",
      "20                NaN  \n",
      "21                NaN  \n",
      "22                NaN  \n",
      "23                NaN  \n",
      "24                NaN  \n",
      "25                NaN  \n",
      "26                NaN  \n",
      "27                NaN  \n",
      "28                NaN  \n",
      "29                NaN  \n",
      "...               ...  \n",
      "1311              NaN  \n",
      "1312              NaN  \n",
      "1313              NaN  \n",
      "1314              NaN  \n",
      "1315              NaN  \n",
      "1316              NaN  \n",
      "1317              NaN  \n",
      "1318              NaN  \n",
      "1319              NaN  \n",
      "1320              NaN  \n",
      "1321              NaN  \n",
      "1322              NaN  \n",
      "1323              NaN  \n",
      "1324              NaN  \n",
      "1325              NaN  \n",
      "1326              NaN  \n",
      "1327              NaN  \n",
      "1328              NaN  \n",
      "1329              NaN  \n",
      "1330              NaN  \n",
      "1331              NaN  \n",
      "1332              NaN  \n",
      "1333              NaN  \n",
      "1334              NaN  \n",
      "1335              NaN  \n",
      "1336              NaN  \n",
      "1337              NaN  \n",
      "1338              NaN  \n",
      "1339              NaN  \n",
      "1340              NaN  \n",
      "\n",
      "[1341 rows x 5 columns]\n",
      "\n",
      "Missing values counting:\n",
      "time                  0\n",
      "call_count            0\n",
      "status_code_2xx      67\n",
      "status_code_4xx     173\n",
      "status_code_5xx    1333\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "start_timestamp = 1530057600\n",
    "end_timestamp = 1530316800\n",
    "\n",
    "service_name = 'nova-api-cascading'\n",
    "\n",
    "metric_name = 'huawei.call_count.{}'.format(service_name)\n",
    "metric_name_2 = 'huawei.status_code.{}.2XX'.format(service_name)\n",
    "metric_name_3 = 'huawei.status_code.{}.4XX'.format(service_name)\n",
    "metric_name_4 = 'huawei.status_code.{}.5XX'.format(service_name)\n",
    "\n",
    "metrics = get_metrics(metric_name, start_timestamp, end_timestamp)\n",
    "metrics_2 = get_metrics(metric_name_2, start_timestamp, end_timestamp)\n",
    "metrics_3 = get_metrics(metric_name_3, start_timestamp, end_timestamp)\n",
    "metrics_4 = get_metrics(metric_name_4, start_timestamp, end_timestamp)\n",
    "\n",
    "# print(len(metrics.items()))\n",
    "# print(len(metrics_2.items()))\n",
    "# print(len(metrics_3.items()))\n",
    "# print(len(metrics_4.items()))\n",
    "\n",
    "df_1 = pd.DataFrame(metrics.items(), columns=['time', 'call_count'])\n",
    "df_2 = pd.DataFrame(metrics_2.items(), columns=['time', 'status_code_2xx'])\n",
    "df_3 = pd.DataFrame(metrics_3.items(), columns=['time', 'status_code_4xx'])\n",
    "df_4 = pd.DataFrame(metrics_4.items(), columns=['time', 'status_code_5xx'])\n",
    "\n",
    "df = pd.merge(df_1, df_2, how='outer')\n",
    "df = pd.merge(df, df_3, how='outer')\n",
    "df = pd.merge(df, df_4, how='outer')\n",
    "\n",
    "print('\\nData info:\\n{}'.format(df.info()))\n",
    "\n",
    "print('\\nData:\\n{}'.format(df))\n",
    "\n",
    "print('\\nMissing values counting:\\n{}'.format(df.isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "\n",
    "In this code block, for visualization purposes, we plot some data retrieved from the time-series database.\n",
    "\n",
    "As we can see, there are missing points in some charts. To perform a good analysis, this problem must be solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Time Series of status_code_5xx')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df.plot(x='time', y='call_count', figsize=(12,6))\n",
    "plt.xlabel('Timestamps')\n",
    "plt.ylabel('Call count')\n",
    "plt.title('Time Series of Call Count')\n",
    "\n",
    "df.plot(x='time', y='status_code_2xx', figsize=(12,6))\n",
    "plt.xlabel('Timestamps')\n",
    "plt.ylabel('status_code_2xx')\n",
    "plt.title('Time Series of status_code_2xx')\n",
    "\n",
    "df.plot(x='time', y='status_code_4xx', figsize=(12,6))\n",
    "plt.xlabel('Timestamps')\n",
    "plt.ylabel('status_code_4xx')\n",
    "plt.title('Time Series of status_code_4xx')\n",
    "\n",
    "df.plot(x='time', y='status_code_5xx', figsize=(12,6))\n",
    "plt.xlabel('Timestamps')\n",
    "plt.ylabel('status_code_5xx')\n",
    "plt.title('Time Series of status_code_5xx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values -- Pain :'(\n",
    "\n",
    "Missing values are a pain in data analysis because we can not perform a good analysis of the data with missing values, and sometimes we do not know clearly what is the best option to fulfill the data. This values are represented by NaN in the dataframes. We can not use any clustering in dataframes with this kind of values.\n",
    "\n",
    "Approaches:\n",
    "- Remove rows with missing values (Degrades the overall data and may result in insufficient data);\n",
    "- Impute missing values (May be dangerous because it introduces \"wrong\" observations);\n",
    "\n",
    "For this example, and because we have a lot of missing values, the best option is to impute missing values, otherwise, if we remove the rows with the missing values it would cause insufficiency in the data which in turn results in inefficient training of the machine learning model.\n",
    "\n",
    "Now, there are several ways we can perform the imputation:\n",
    "- A constant value that has meaning within the domain, such as 0, distinct from all other values;\n",
    "- A value from another randomly selected record;\n",
    "- A mean, median or mode value for the column;\n",
    "- Linear interpolation;\n",
    "- A value estimated by another machine learning model.\n",
    "\n",
    "[How to handle missing data in Time-Series](https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4)\n",
    "\n",
    "To know better the method to use to fulfill the missing values, we perform a decomposition of data to perceive how is the Trend, Seasonality and Residuals in our data.\n",
    "\n",
    "From the charts displayed bellow, we can notice that our data only represents trends. From the link provided above, through the path \"Handling Missing Data --> Imputation --> Time-Series Problem --> Data with Trend and without Seasonality\", we can select \"Linear Interpolation\" as the best method to fulfill our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cb940685ae40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseasonal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseasonal_decompose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseasonal_decompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'call_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'additive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "from pandas import Series\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "result = seasonal_decompose(df['call_count'].dropna(), model='additive', freq=1)\n",
    "result.plot()\n",
    "pyplot.show()\n",
    "\n",
    "result = seasonal_decompose(df['status_code_2xx'].dropna(), model='additive', freq=1)\n",
    "result.plot()\n",
    "pyplot.show()\n",
    "\n",
    "result = seasonal_decompose(df['status_code_4xx'].dropna(), model='additive', freq=1)\n",
    "result.plot()\n",
    "pyplot.show()\n",
    "\n",
    "result = seasonal_decompose(df['status_code_5xx'].dropna(), model='additive', freq=1)\n",
    "result.plot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values fulfill\n",
    "\n",
    "As explained before, with time-series data decomposed and showing only sines of Trend without Seasonality, the best option to fulfill data is to perform a linear interpolation on the data. From this, bellow is a piece of code that applies linear interpolation to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_copy.copy()\n",
    "\n",
    "df = df.interpolate(method ='linear')\n",
    "\n",
    "df_copy.plot(x='time', y='status_code_2xx', figsize=(12,6))\n",
    "plt.xlabel('Timestamps')\n",
    "plt.ylabel('status_code_2xx')\n",
    "plt.title('Time Series of status_code_2xx (MISSING_VALUES)')\n",
    "\n",
    "df.plot(x='time', y='status_code_2xx', figsize=(12,6))\n",
    "plt.xlabel('Timestamps')\n",
    "plt.ylabel('status_code_2xx')\n",
    "plt.title('Time Series of status_code_2xx (COMPLETE)')\n",
    "\n",
    "df.plot(x='time', y='status_code_4xx', figsize=(12,6))\n",
    "plt.xlabel('Timestamps')\n",
    "plt.ylabel('status_code_4xx')\n",
    "plt.title('Time Series of status_code_4xx (COMPLETE)')\n",
    "\n",
    "df.plot(x='time', y='status_code_5xx', figsize=(12,6))\n",
    "plt.xlabel('Timestamps')\n",
    "plt.ylabel('status_code_5xx')\n",
    "plt.title('Time Series of status_code_5xx (COMPLETE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df['status'] = np.where(df['status_code_2xx'] > df['status_code_4xx'] + df['status_code_5xx'], 1, 0)\n",
    "\n",
    "df['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data = df[['call_count', 'status']]\n",
    "n_cluster = range(1, 10)\n",
    "kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\n",
    "scores = [kmeans[i].score(data) for i in range(len(kmeans))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(n_cluster, scores)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data = df[['call_count', 'status_code_2xx', 'status_code_4xx', 'status_code_5xx']]\n",
    "n_cluster = range(1, 10)\n",
    "kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\n",
    "scores = [kmeans[i].score(data) for i in range(len(kmeans))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(n_cluster, scores)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "X = df[['call_count', 'status_code_2xx', 'status_code_4xx', 'status_code_5xx']]\n",
    "X = X.reset_index(drop=True)\n",
    "km = KMeans(n_clusters=3.5)\n",
    "km.fit(X)\n",
    "km.predict(X)\n",
    "labels = km.labels_\n",
    "#Plotting\n",
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)\n",
    "ax.scatter(X.iloc[:,0], X.iloc[:,1], X.iloc[:,2],\n",
    "          c=labels.astype(np.float), edgecolor=\"k\")\n",
    "ax.set_xlabel(\"price_usd\")\n",
    "ax.set_ylabel(\"srch_booking_window\")\n",
    "ax.set_zlabel(\"srch_saturday_night_bool\")\n",
    "plt.title(\"K Means\", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = df[['call_count', 'status']]\n",
    "X = data.values\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
    "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(var_exp)), var_exp, alpha=0.3, align='center', label='individual explained variance', color = 'g')\n",
    "plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc='best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = df[['call_count', 'status_code_2xx', 'status_code_4xx', 'status_code_5xx']]\n",
    "X = data.values\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
    "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(var_exp)), var_exp, alpha=0.3, align='center', label='individual explained variance', color = 'g')\n",
    "plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc='best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistanceByPoint(data, model):\n",
    "    distance = pd.Series()\n",
    "    for i in range(0,len(data)):\n",
    "        Xa = np.array(data.loc[i])\n",
    "        Xb = model.cluster_centers_[model.labels_[i]-1]\n",
    "        distance.set_value(i, np.linalg.norm(Xa-Xb))\n",
    "    return distance\n",
    "\n",
    "outliers_fraction = 0.01\n",
    "# get the distance between each point and its nearest centroid. The biggest distances are considered as anomaly\n",
    "distance = getDistanceByPoint(data, kmeans[8])\n",
    "number_of_outliers = int(outliers_fraction*len(distance))\n",
    "threshold = distance.nlargest(number_of_outliers).min()\n",
    "# anomaly1 contain the anomaly result of the above method Cluster (0:normal, 1:anomaly) \n",
    "df['anomaly1'] = (distance >= threshold).astype(int)\n",
    "\n",
    "# visualisation of anomaly with cluster view\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "colors = {0:'blue', 1:'red'}\n",
    "ax.scatter(df['call_count'], df['status'], c=df[\"anomaly1\"].apply(lambda x: colors[x]))\n",
    "plt.xlabel('principal feature1')\n",
    "plt.ylabel('principal feature2')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('time')\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "a = df.loc[df['anomaly1'] == 1, ['time', 'call_count']] #anomaly\n",
    "\n",
    "ax.plot(df['time'], df['call_count'], color='blue', label='Normal')\n",
    "ax.scatter(a['time'],a['call_count'], color='red', label='Anomaly')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Call count')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "a = df.loc[df['anomaly1'] == 1, ['time', 'status']] #anomaly\n",
    "\n",
    "ax.plot(df['time'], df['status'], color='blue', label='Normal')\n",
    "ax.scatter(a['time'],a['status'], color='red', label='Anomaly')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Status')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
